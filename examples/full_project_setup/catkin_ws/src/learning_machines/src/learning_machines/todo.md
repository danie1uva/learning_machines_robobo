## TO DO
1. Fix bug in how rewards/loss is logged in run_DQN. currently using np.savez, it was a chatgpt rec, if something else works go for that. i think it's a good idea to use a similar structure to what we did in week 1 around saving results/figures/plots, so add a subfolder week 2 to results folder, and then a subsequent models, runlogs folders. can check the week 1 script to see the syntax for that. 
1. Silly but before we do anything else, let the robot run as is for a long time, say 1000 rounds, max 200 steps per episode and see that it actually learns something: overfitting would be fine at this stage since there's no variation. if step 1 hasn't been fixed first this will fail at the end so fix logging bug first.
2. Add randomness to robot episodic starting position: ideally 3 diff starting places. we could consider rotating the car too for more variation. this is essentially model validation. maybe we treat the 3 starting locations as train/validate/test.
3. think about how we want to stop training. maybe we do something like: perhaps for every 20 rounds we run from the original starting position, then run from validation and once performance (measured by loss or reward?) on this stabilises, check perfomance on test.
3. We should also think about how we'll plot our reward/loss logs, as this will give us an insight into performance. note the way i currently compute the reward as a collection of things to discourage/encourage. it's likely we'll need to weight these different pieces differently as is done now, but that'll only become apparent when we observe how the model evolves. 